{
  "__inputs": [
    {
      "name": "DS_PROMETHEUS-LOCAL",
      "label": "prometheus-local",
      "description": "",
      "type": "datasource",
      "pluginId": "prometheus",
      "pluginName": "Prometheus"
    }
  ],
  "__elements": {},
  "__requires": [
    {
      "type": "grafana",
      "id": "grafana",
      "name": "Grafana",
      "version": "11.6.0"
    },
    {
      "type": "datasource",
      "id": "prometheus",
      "name": "Prometheus",
      "version": "1.0.0"
    },
    {
      "type": "panel",
      "id": "timeseries",
      "name": "Time series",
      "version": ""
    }
  ],
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": null,
  "links": [],
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "GC collections/s:\nSuma z rate() licznika kolekcji GC daje liczbę cykli garbage collectora na sekundę, zsumowaną przez wszystkie generacje. Jeśli wartości są wyższe na jednej platformie, oznacza to, że tam GC wykonuje się częściej. Np. aplikacja na GCP może wykonać 2 cykle/s, a na Azure 1 cykl/s – co sugeruje, że na GCP obiektów przybywa szybciej (lub mniej pamięci, więc GC musi sprzątać częściej). Wykres pozwoli zaobserwować też wzorce okresowe (Python GC generacji 2 może uruchamiać się co pewien czas).\n\nObjects Collected:\nPokazuje, ile obiektów średnio usuwa GC w jednostce czasu. Jeśli np. na Azure jest dużo wyższa liczba, może to oznaczać, że aplikacja tam tworzy i usuwa więcej obiektów (np. inna implementacja jakiejś biblioteki, generująca więcej śmieci), albo że GC czeka dłużej i zbiera więcej naraz. Porównanie z poprzednią metryką jest ciekawe – np. więcej cykli, ale mniej obiektów vs mniej cykli, a więcej obiektów na cykl. To wskaże różnicę w zachowaniu pamięciowym aplikacji między środowiskami.\n\nGC objects collected/s:\nPokazuje, ile obiektów średnio usuwa GC w jednostce czasu. Jeśli np. na Azure jest dużo wyższa liczba, może to oznaczać, że aplikacja tam tworzy i usuwa więcej obiektów (np. inna implementacja jakiejś biblioteki, generująca więcej śmieci), albo że GC czeka dłużej i zbiera więcej naraz. Porównanie z poprzednią metryką jest ciekawe – np. więcej cykli, ale mniej obiektów vs mniej cykli, a więcej obiektów na cykl. To wskaże różnicę w zachowaniu pamięciowym aplikacji między środowiskami.\n\nFull GC runs/s:\nTe wykresy zilustrują efektywność zarządzania pamięcią: stabilna, niska częstotliwość GC jest pożądana (mniej przerw), ale zbyt rzadki GC przy gwałtownie rosnącej pamięci też jest zły. Jeśli np. na Azure widać gwałtowne zrywy (długo nic, potem dużo obiektów na raz), a na GCP częstsze małe czyszczenia – można to opisać jako różnicę strategii (choć GC Python powinien działać tak samo, czasem inne biblioteki mogą ingerować). W kontekście pracy magisterskiej, większa częstotliwość GC może sugerować większą presję pamięciową lub częstsze alokacje, co warto powiązać z poprzednimi metrykami pamięci.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 9,
        "w": 16,
        "x": 0,
        "y": 0
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(python_gc_collections_total[$graph_interval])\r\n)\r\n",
          "legendFormat": "GC collections/s - {{provider}}",
          "range": true,
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          }
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(python_gc_objects_collected_total[$graph_interval])\r\n)",
          "hide": false,
          "instant": false,
          "legendFormat": "GC objects collected/s - {{provider}}",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n    rate(python_gc_collections_total{generation=\"2\"}[$graph_interval])\r\n)",
          "hide": false,
          "instant": false,
          "legendFormat": "Full GC runs/s - {{provider}}",
          "range": true,
          "refId": "C"
        }
      ],
      "title": "Garbage Collection (Zarządzanie pamięcią w Pythonie)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "CPU usage (cores):\nBierzemy pochodną czasu CPU z ostatniej minuty i uśredniamy po instancjach w danej chmurze. Wynik to przeciętne użycie CPU (w jednostkach “rdzeni”) na jedną instancję. Np. wartość 0.5 oznacza ~50% jednego rdzenia. Jeśli architektura jest taka sama, oczekujemy podobnego zużycia CPU przy tym samym ruchu. Różnice mogą wynikać z wydajności sprzętu: np. jeśli na GCP proces zużywa 0.4 a na Azure 0.6, może to znaczyć, że instancje na Azure są wolniejsze (więc potrzebują więcej CPU), albo kod wykonuje się mniej efektywnie (np. inny kernel, inny overhead). W Grafanie wykres (linia dla GCP, linia dla Azure) pozwoli prześledzić trend – np. czy pod szczytowym obciążeniem obie dochodzą do 1.0 (100% jednego rdzenia) czy któraś przekracza (przy wielowątkowości może przekroczyć 1.0 jeśli więcej rdzeni używa).\n\nResident memory (bytes):\nŚrednia pamięć rezydentna (fizyczna) per instancja na każdej platformie. Jeśli np. wykres Azure wyświetla ~500MiB a GCP ~400MiB, to znaczy że aplikacja na Azure zajmuje więcej pamięci. To może wynikać z różnic w systemie (np. alokator pamięci), albo z tego, że ruch/testy spowodowały załadowanie innych danych. Wykres w czasie pozwoli zobaczyć trendy – np. czy pamięć stale rośnie (potencjalny leak) tylko na jednej platformie.\n\nVirtual memory (VSZ):\nWirtualna pamięć obejmuje zaalokowane adresy (mogą być nieużywane). Często jest dużo większa niż RSS. Różnice między GCP a Azure mogą wynikać z innych bibliotek lub sposobu mapowania plików. Na potrzeby porównania stabilności, RSS jest ważniejszy (pokazuje rzeczywiste zużycie). VSZ można monitorować, ale jeśli obie rosną bez opamiętania – to też wskazówka potencjalnego problemu.\n\nOpen FDs:\nPokazuje ile plików/gniazd sieciowych ma otwarty proces. W środowiskach serwerowych limit FD bywa istotny. Jeśli jedna platforma pokazuje systematycznie wyższe FD (np. Azure 150, GCP 100), może to oznaczać inaczej działający pooling połączeń, inaczej obsługiwane pliki statyczne itp. Ważne są zmiany w czasie – np. rosnąca liczba FD może wskazywać na wycieki deskryptorów (niezamykane połączenia), co prędzej czy później spowoduje awarię. Porównanie pozwoli stwierdzić czy problem występuje na obu, czy np. tylko na jednej platformie (co sugeruje specyficzny bug lub konfigurację tamże).",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": [
          {
            "__systemRef": "hideSeriesFrom",
            "matcher": {
              "id": "byNames",
              "options": {
                "mode": "exclude",
                "names": [
                  "CPU usage (cores) - local"
                ],
                "prefix": "All except:",
                "readOnly": true
              }
            },
            "properties": [
              {
                "id": "custom.hideFrom",
                "value": {
                  "legend": false,
                  "tooltip": false,
                  "viz": true
                }
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 10,
        "w": 16,
        "x": 0,
        "y": 9
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "editorMode": "code",
          "expr": "avg by(provider) (\r\n  rate(process_cpu_seconds_total{provider=~\".+\"}[$graph_interval])\r\n)\r\n",
          "legendFormat": "CPU usage (cores) - {{provider}}",
          "range": true,
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          }
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "avg by(provider) (\r\n  process_resident_memory_bytes{provider=~\".+\"}\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Resident memory (bytes) - {{provider}}",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "avg by(provider) (\r\n  process_virtual_memory_bytes{provider=~\".+\"}\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Virtual memory (bytes) - {{provider}}",
          "range": true,
          "refId": "C"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "avg by(provider) (\r\n  process_open_fds{provider=~\".+\"}\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Open FDs - {{provider}}",
          "range": true,
          "refId": "D"
        }
      ],
      "title": "Metryki systemowe (CPU, pamięć, deskryptory plików)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "Request Rate (RPS):\nSuma z rate() licznika żądań GET daje średnią liczbę zapytań GET na sekundę w oknie 5 minut dla każdej chmury. Jeśli aplikacja jest pod podobnym obciążeniem na obu platformach, wykresy powinny się pokrywać. Rozbieżności mogą wskazywać np. na throttling lub wolniejsze przetwarzanie – np. jeśli Azure obsługuje mniej RPS niż GCP przy tym samym teście, może to oznaczać wąskie gardło.\n\n200 OK Response Rate:\nTo zapytanie pokazuje ile poprawnych odpowiedzi (HTTP 200) na sekundę zwraca aplikacja na każdej platformie. Można je zestawić z RPS żądań – w idealnym scenariuszu linia 200 OK pokrywa się z linią żądań (czyli większość żądań kończy się sukcesem). Jeśli np. w Azure linia 200/s jest zauważalnie poniżej linii żądania/s, to oznacza większy odsetek błędów (np. część żądań nie kończy się 200).\n\nError rate:\nWykres (linii błędu w %) od razu pokaże, która platforma częściej zwraca błędy (np. timeouty lub 5xx). Niższa wartość oznacza lepszą stabilność.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 16,
        "x": 0,
        "y": 19
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_requests_total_by_method_total{method=\"GET\"}[$graph_interval])\r\n)\r\n",
          "format": "time_series",
          "legendFormat": "[Request Rate (RPS)] GET requests/s - {{provider}}",
          "range": true,
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          }
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_responses_total_by_status_total{status=\"200\"}[$graph_interval])\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "[200 OK Response Rate] 200 OK responses/s - {{provider}}",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "100 * (1 - (sum(rate(django_http_responses_total_by_status_total{status=~\"2..\"}[$graph_interval])) by(provider) \r\n           / sum(rate(django_http_requests_total_by_method_total[$graph_interval])) by(provider) ))\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Error rate (%) - {{provider}}",
          "range": true,
          "refId": "C"
        }
      ],
      "title": "Liczba żądań i odpowiedzi (Throughput i kod statusu)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "Average Request Size:\nDzielimy tempo przyrostu sumy bajtów przez tempo przyrostu liczby żądań (w tym samym oknie). Otrzymujemy średnią wielkość requestu w bajtach dla każdej chmury. Np. jeśli średni rozmiar żądania w Azure jest wyższy, może to oznaczać, że klienci (lub testy) kierują tam inne zapytania, albo że Azure nie kompresuje tak bardzo danych przy przesyłaniu (choć w przypadku requestów zazwyczaj to klient wysyła dane).\n\n90th percentile Response Size:\nKorzystamy z histogramu rozmiarów odpowiedzi, by wyliczyć wartość, poniżej której mieści się 90% odpowiedzi (dla każdej platformy). Ten percentyl pokazuje nam, jak duże są większe odpowiedzi. Jeśli np. na wykresie 90p dla GCP wynosi ~500KB, a dla Azure ~300KB, znaczy to że w GCP zdarzają się cięższe odpowiedzi (być może inne zachowanie cache, albo większe zestawy danych zwracane z bazy).\n\nAvg Request Body Size (bytes):\nTo zapytanie oblicza średni rozmiar ciała żądania (w bajtach) dla każdej grupy określonej etykietą cloud_provider.\n\n90p Request Body Size (bytes):\nZapytanie to wylicza 90. percentyl rozmiaru ciała żądania – tzn. 90% żądań ma rozmiar mniejszy lub równy uzyskanej wartości, co pozwala ocenić, czy np. jedna platforma generuje większe payloady niż inna.\n\nAvg Response Body Size (bytes):\nTo zapytanie wylicza średni rozmiar ciała odpowiedzi w bajtach dla każdej grupy cloud_provider.\n\n90p Response Body Size (bytes):\nTo zapytanie oblicza 90. percentyl rozmiaru odpowiedzi – 90% odpowiedzi ma rozmiar mniejszy lub równy tej wartości.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 11,
        "w": 16,
        "x": 0,
        "y": 27
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "editorMode": "code",
          "expr": "sum(rate(django_http_requests_body_total_bytes_sum[$graph_interval])) by(provider)/\r\nsum(rate(django_http_requests_body_total_bytes_count[$graph_interval])) by(provider)",
          "legendFormat": "Avg request size (bytes) - {{provider}}",
          "range": true,
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          }
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.90, sum by(provider, le) (\r\n  rate(django_http_responses_body_total_bytes_bucket[$graph_interval])\r\n))",
          "hide": false,
          "instant": false,
          "legendFormat": "90p response size (bytes) - {{provider}}",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_requests_body_total_bytes_sum[$graph_interval])\r\n)\r\n/\r\nsum by(provider) (\r\n  rate(django_http_requests_body_total_bytes_count[$graph_interval])\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Avg Request Body Size (bytes) - {{provider}}",
          "range": true,
          "refId": "C"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(\r\n  0.90,\r\n  sum by(provider, le) (\r\n    rate(django_http_requests_body_total_bytes_bucket[$graph_interval])\r\n  )\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "90p Request Body Size (bytes) - {{provider}}",
          "range": true,
          "refId": "D"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_responses_body_total_bytes_sum[$graph_interval])\r\n)\r\n/\r\nsum by(provider) (\r\n  rate(django_http_responses_body_total_bytes_count[$graph_interval])\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Avg Response Body Size (bytes) - {{provider}}",
          "range": true,
          "refId": "E"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(\r\n  0.90,\r\n  sum by(provider, le) (\r\n    rate(django_http_responses_body_total_bytes_bucket[$graph_interval])\r\n  )\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "90p Response Body Size (bytes) - {{provider}}",
          "range": true,
          "refId": "F"
        }
      ],
      "title": "Rozmiar żądań/odpowiedzi (Payload Size)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "Not Found (404) exceptions rate:\nTo zapytanie pokazuje średnią liczbę 404-ek na sekundę w ciągu ostatnich $graph_interval dla każdej platformy. Jeśli np. wykres dla Azure jest wyższy, oznacza to, że aplikacja na Azure częściej zwraca 404 (być może różnice w konfiguracji, routing, pliki statyczne itp.). Generalnie 404 nie musi oznaczać problemu wydajności, ale daje wgląd w różnice funkcjonalne/stabilnościowe między środowiskami (np. czy obie platformy są skonfigurowane tak samo).\n\nExceptions (all types):\nTo pokaże ogólne tempo błędów aplikacji. Jeśli jedna chmura wykazuje więcej wyjątków ogółem, może to oznaczać problemy specyficzne dla tego środowiska (np. błędy integracji, różnice w danych).",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 11,
        "w": 16,
        "x": 0,
        "y": 38
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_exceptions_total_by_type_total{type=\"Http404\"}[$graph_interval])\r\n)\r\n",
          "legendFormat": "[Not Found (404) exceptions rate] Http404 exceptions/s - {{provider}}",
          "range": true,
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          }
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_exceptions_total_by_type_total[$graph_interval])\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Exceptions (all types) - {{provider}}",
          "range": true,
          "refId": "B"
        }
      ],
      "title": "Obsługa wyjątków (Exception Handling)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "DS_PROMETHEUS-LOCAL"
      },
      "description": "Na tym panelu znajdują się różne miary latencji obsługi żądań w aplikacji Django, mierzone przez biblioteki Prometheus/django-prometheus. Dzięki temu możesz śledzić, jak szybko aplikacja odpowiada i czy pojawiają się wąskie gardła przy wzroście obciążenia.\n\nAverage Latency (s) – local\n- Średnia (mean) wszystkich czasów odpowiedzi żądań w danym przedziale (np. 5 minut).\n- Dobrze oddaje „ogólny” czas obsługi, ale bywa zaniżona, gdy część żądań jest znacznie wolniejsza.\n\n95th Percentile Latency (s) – local:\n- Percentyl 95 oznacza, że 95% żądań kończy się szybciej niż ta wartość, a 5% – wolniej.\n- Ta miara lepiej odzwierciedla „gorsze przypadki” niż sama średnia.\n- Jeśli 95p rośnie, oznacza to, że rośnie liczba wolnych żądań.\n\nStore view p90 Latency (s) – local:\n- 90 percentyl dla konkretnego widoku w Django (np. store) i metody (np. GET).\n- Pozwala zobaczyć, jak szybko obsługiwane są żądania w krytycznym endpointzie.\n- Przydatne do szczegółowej analizy jednego widoku, np. strony sklepu.\n\nLatency bucket ≤ X (s) – local:\n- Seria linii oznaczających wiaderka (bucket) w histogramie. Każda odpowiada liczbie (lub przyroście liczby) żądań obsłużonych w czasie ≤ X sekund.\n- Przykład: „Latency bucket ≤ 1.0 (s)” oznacza, ile żądań zakończyło się w czasie do 1 sekundy.\n- Wykres rosnący w czasie wskazuje, że rośnie (kumulatywnie) liczba żądań obsłużonych w danym przedziale latencji.\n- Może służyć do analizy rozkładu czasów – np. czy większość żądań mieści się w 0,1 s czy w 1 s.\n\nJak interpretować wzrost wykresów:\n- Jeśli średnia i percentyl (np. 95p) rosną, oznacza to, że ogólnie aplikacja zaczyna odpowiadać wolniej (wzrost latencji) – prawdopodobnie rośnie obciążenie lub wystąpił problem z zasobami.\n- Duża różnica między średnią a 95p sugeruje, że spora część żądań jest szybka, ale pewien odsetek jest znacznie wolniejszy (tzw. long-tail).\n- Linie bucketów powyżej 2–3 s rosnące szybciej mogą wskazywać, że coraz więcej żądań trafia do wolnych przedziałów czasu (np. powyżej 3 s).",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 13,
        "w": 16,
        "x": 0,
        "y": 49
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider) (\r\n  rate(django_http_requests_latency_including_middlewares_seconds_sum[$graph_interval])\r\n)\r\n/\r\nsum by(provider) (\r\n  rate(django_http_requests_latency_including_middlewares_seconds_count[$graph_interval])\r\n)",
          "hide": false,
          "instant": false,
          "legendFormat": "Average Latency (s) - {{provider}}",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "exemplar": false,
          "expr": "histogram_quantile(\r\n  0.95,\r\n  sum by(provider, le) (\r\n    rate(django_http_requests_latency_including_middlewares_seconds_bucket[$graph_interval])\r\n  )\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "95th Percentile Latency (s) - {{provider}}",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "expr": "sum by(provider, le) (\r\n  rate(django_http_requests_latency_including_middlewares_seconds_bucket[$graph_interval])\r\n)",
          "hide": false,
          "instant": false,
          "legendFormat": "Latency bucket ≤ {{le}} (s) - {{provider}}",
          "range": true,
          "refId": "C"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "DS_PROMETHEUS-LOCAL"
          },
          "editorMode": "code",
          "exemplar": false,
          "expr": "histogram_quantile(\r\n  0.90, \r\n  sum by(provider, le) (\r\n    rate(django_http_requests_latency_seconds_by_view_method_bucket{view=\"store\", method=\"GET\"}[$graph_interval])\r\n  )\r\n)\r\n",
          "hide": false,
          "instant": false,
          "legendFormat": "Store view p90 latency (s) - {{provider}}",
          "range": true,
          "refId": "D"
        }
      ],
      "title": "Czas przetwarzania żądań (Request Processing Time)",
      "type": "timeseries"
    }
  ],
  "schemaVersion": 41,
  "tags": [],
  "templating": {
    "list": [
      {
        "allowCustomValue": true,
        "current": {
          "text": "1h",
          "value": "1h"
        },
        "name": "graph_interval",
        "options": [
          {
            "selected": true,
            "text": "1h",
            "value": "1h"
          },
          {
          "text": "6h",
          "value": "6h",
          "selected": false
          },
        {
          "text": "12h",
          "value": "12h",
          "selected": false
        },
        {
          "text": "24h",
          "value": "24h",
          "selected": false
        }
        ],
        "query": "1h",
        "type": "custom"
      }
    ]
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "browser",
  "title": "Panele Grafany do porównania usług chmurowych",
  "uid": "eehjbl7oak1s0d",
  "version": 63,
  "weekStart": ""
}